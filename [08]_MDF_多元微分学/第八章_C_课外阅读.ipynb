{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d34b27c-f85c-460d-828b-4e095a6e3772",
   "metadata": {},
   "source": [
    "## 1. 可微映射的雅可比矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1671a6d7-190c-48c9-b7de-fae1dc14f4f2",
   "metadata": {},
   "source": [
    "1. 全微分是函数在一点的线性逼近，全微分是一个线性函数。\n",
    "2. 切平面是全微分的几何实现，是线性函数 $ \\mathrm{d}f\\colon\\mathbb{R}^2\\to\\mathbb{R}$的图像。\n",
    "3. 一般的，对于高维空间的映射，即向量值函数$$\\mathbf{f}\\colon\\mathbb{R}^n\\to\\mathbb{R}^m,$$$$\\left(\\begin{array}{c}x_1\\\\x_2\\\\ \\cdots\\\\x_n\\end{array}\\right)\\to \\left(\\begin{array}{c}f_1(x_1,\\cdots,x_n)\\\\f_2(x_1,\\cdots,x_m)\\\\ \\cdots\\\\f_m(x_1,\\cdots,x_n)\\end{array}\\right)$$它的线性逼近是什么呢？是一个线性映射，可以用矩阵来表示\n",
    "   $$\\mathrm{d}\\mathbf{f}=\\left(\\begin{array}{cc}\\frac{\\partial f_1}{\\partial x_1}&\\frac{\\partial f_1}{\\partial x_2}&\\cdots\\\\ \\frac{\\partial f_2}{\\partial x_1}&\\frac{\\partial f_2}{\\partial x_2}&\\cdots\\\\\n",
    "   \\cdots&\\cdots&\\cdots\\\\\n",
    "   \\frac{\\partial f_m}{\\partial x_1}&\\frac{\\partial f_m}{\\partial x_2}&\\cdots \\end{array}\\right)$$\n",
    "4. 如果$m=n$, 即$\\mathrm{d}\\mathbf{f}$ 是方阵，我们称其行列式为映射在点$\\mathbf{x}$的雅可比行列式（Jacobian），$$J_{\\mathbf{x}}(\\mathbf{f})=\\mathrm{det}(\\mathrm{d}\\mathbf{f}|_{\\mathbf{x}})$$它表示线性映射在这一点体积微元在映射前后的缩放率。\n",
    "5. 考虑$$\\mathbf{f}\\colon\\mathbb{R}^2\\to\\mathbb{R}^2,\\ (r,\\theta)\\mapsto(r\\cos\\theta,r\\sin\\theta)$$则$$\\mathrm{d}\\mathbf{f}=\\left(\\begin{array}{cc}\\cos\\theta& -r\\sin\\theta\\\\ \\sin\\theta& r\\cos\\theta\\end{array}\\right)$$\n",
    "   计算得到$$J(\\mathbf{f})=r$$因此体积微元$$\\mathrm{d}x\\wedge\\mathrm{d}y=r\\mathrm{d}r\\wedge\\mathrm{d}\\theta$$\n",
    "   这里$\\wedge$表示定向的面积乘法，二元就是叉乘，计算面积，高维是叉乘的推广，这个记号满足线性性和反交换性$$X\\wedge a\\wedge b\\wedge Y=-X\\wedge  b\\wedge a\\wedge Y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e512db3-ecb2-4bb3-be8e-b5e7ed6af085",
   "metadata": {},
   "source": [
    "参考 [金路、徐惠平]高等数学同步辅导与复习提高（第三版） 5.4 节。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a61e8c-4e86-4e7d-8391-0b47a3d130b7",
   "metadata": {},
   "source": [
    "## 2. 与机器学习的关系"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ad9d77-471a-40a4-addd-7bc610e899ea",
   "metadata": {},
   "source": [
    "https://playground.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbb22ed-018e-440a-9ce0-f9bb099779af",
   "metadata": {},
   "source": [
    "#### 偏导数在机器学习中的核心作用\n",
    "\n",
    "在机器学习中，优化模型参数是核心任务，而偏导数用于计算梯度（多变量函数的导数），指导参数更新方向。具体应用场景包括：\n",
    "\n",
    "- 梯度下降法（优化损失函数）\n",
    "\n",
    "- 反向传播算法（神经网络参数更新）\n",
    "\n",
    "- 多变量概率模型（如高斯混合模型）\n",
    "\n",
    "- 正则化技术（如L1/L2正则化项的导数计算）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade1ce73-adb8-43f1-a1e2-e8193ff5c459",
   "metadata": {},
   "source": [
    "#### 案例1：线性回归中的梯度下降\n",
    "\n",
    "问题描述：用线性模型拟合数据，最小化均方误差（MSE）\n",
    "\n",
    "- 假设样本点满足函数 $$h_\\theta(x)=\\theta_0+\\theta_1x$$\n",
    "- 损失函数 $$J(\\theta_0,\\theta_1)=\\frac{1}{m}\\sum_{i=1}^m(h(x_i)-y_i)^2$$\n",
    "- 目标：找到使$J(\\theta_0,\\theta_1)$最小的$\\theta_0,\\theta_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d2945-7866-40eb-8625-895f7ad379b3",
   "metadata": {},
   "source": [
    "- 偏导数计算 $$\\frac{\\partial J}{\\partial\\theta_0},\\ \\frac{\\partial J}{\\partial\\theta_0}$$\n",
    "- 参数更新规则$$\\theta_j\\leftarrow\\theta_j-\\alpha\\frac{\\partial J}{\\partial \\theta_j}$$\n",
    "这里参数$\\alpha$是学习率，是欧拉迭代法中的“步长”的类比。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f5b244-b143-4f32-a687-0c7d06a63bf1",
   "metadata": {},
   "source": [
    "#### 案例2：神经网络的反向传播\n",
    "\n",
    "问题描述：训练神经网络时，需计算损失函数对各层权重 $W$ 的偏导数。\n",
    "\n",
    "- **链式法则**：反向传播的核心是复合函数求导的链式法则。\n",
    "\n",
    "- 示例网络结构（简单两层网络）：\n",
    "\n",
    "输入层 $\\rightarrow$ 隐藏层（激活函数$\\sigma$） $\\rightarrow$ 输出层（线性输出）\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2827865c-19ea-43d8-8669-ee84bf69c31c",
   "metadata": {},
   "source": [
    "假设损失函数为$L$, 隐藏层输出函数为$a_i=\\sigma(W_{ij}x_j+b_i)$, 要学习的权重$W_{jk}$。其中\n",
    "- $\\frac{\\partial L}{\\partial a_k}$ 取决于损失函数\n",
    "- $a_k$类似于$\\arctan$, 常取为Sigmoid函数\n",
    "- $\\frac{\\partial z_k}{\\partial W_{jk}}=x_j$，是输入数据\n",
    "\n",
    "则\n",
    "$$\\frac{\\partial L}{\\partial W_{jk}}=\\sum_{i}\\frac{\\partial L}{\\partial a_i}\\cdot\\frac{\\partial a_i}{\\partial z_j}\\cdot\\frac{\\partial z_j}{\\partial x_k}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84013e15-e2d4-4f92-a8cd-6b294561c39c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
